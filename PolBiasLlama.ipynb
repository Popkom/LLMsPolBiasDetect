{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241dc928-cce4-45ce-8b36-205580badce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (4.65.0)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.10.0)\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (0.20.2)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.10.0)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from peft==0.10.0)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.10.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.10.0) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, accelerate, peft\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.1\n",
      "    Uninstalling safetensors-0.4.1:\n",
      "      Successfully uninstalled safetensors-0.4.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "Successfully installed accelerate-1.4.0 huggingface-hub-0.29.1 peft-0.10.0 safetensors-0.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.4.0\n",
      "    Uninstalling accelerate-1.4.0:\n",
      "      Successfully uninstalled accelerate-1.4.0\n",
      "Successfully installed accelerate-0.34.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface-hub in /opt/miniconda3/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (0.5.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2023.11.17)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.20.3 transformers-4.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers==3.1.1\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (10.0.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.1.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.1.1) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.1.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.1.1) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.29.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.7.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from bitsandbytes) (1.26.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.10.0\n",
    "!pip install accelerate==0.34.2\n",
    "!pip install -U huggingface-hub\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==4.45.2\n",
    "!pip install sentence-transformers==3.1.1\n",
    "!pip install protobuf\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6d00e-bff7-42a2-9bc3-e11ffec2f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 article       label\n",
      "0      If Donald Trump loses November’s election, it ...        left\n",
      "1      Donald Trump is not a rational choice for cons...        left\n",
      "2      The outsider candidate has money, a running ma...        left\n",
      "3      No recent Democratic president has faced such ...        left\n",
      "4      The conservative justices have shown they are ...        left\n",
      "...                                                  ...         ...\n",
      "13898  ATLANTA — A Republican-led group is challengin...  lean-right\n",
      "13899  PRAIRIE DU CHIEN, Wis. — A day after Vice Pres...  lean-right\n",
      "13900  Donald Trump says newly released statistics fr...  lean-right\n",
      "13901  OPINION: Colombian President Gustavo Petro, in...  lean-right\n",
      "13902  Vice President Kamala Harris is closing out th...  lean-right\n",
      "\n",
      "[13903 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "login(token='secret')\n",
    "\n",
    "with open('dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def shorten_article(entry):\n",
    "    entry[\"article\"] = entry[\"article\"][:int(len(entry[\"article\"])*0.25)]\n",
    "    cleaned_text = re.sub(r'\\b(Donald|donald|Trump|trump|former|Former|president|President|Joe|joe|Biden|biden)\\b', '', entry[\"article\"])\n",
    "    entry[\"article\"] = cleaned_text.strip()\n",
    "    return entry\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#df = df.sample(frac=0.25,random_state=42)\n",
    "print(df)\n",
    "\n",
    "label_mapping = {\"left\": 0, \"lean-left\": 1, \"lean-right\": 2, \"right\": 3}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "train_size = 0.6\n",
    "temp_size = 0.4\n",
    "val_size = 0.5\n",
    "test_size = 0.5\n",
    "df_train, df_temp = train_test_split(df, train_size=train_size, test_size=temp_size, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=val_size, test_size=test_size, random_state=42)\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "dataset_test = Dataset.from_pandas(df_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a31bfe88-550e-4f59-ac59-fe5e303d4f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 8341\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 2781\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 2781\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "    'test': dataset_test\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473b63d-4434-45c9-9257-831ea517aabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "3    0.333893\n",
       "1    0.313512\n",
       "2    0.294689\n",
       "0    0.057907\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130a1b51-4012-4534-9d12-f8519c220404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6432, 0.1188, 0.1264, 0.1116])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (1/df_train.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1cab808-2892-4bb7-a2cc-58fda95218b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec94fbb1-b7cf-4524-9939-d35946b2e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450407f7-fb91-4263-93b2-e23e2f4a3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fbaaf32-1dfc-4caa-83fb-a261747c63d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32b24f653d240ac98c30a6e409820c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe61f11-8895-4d77-a602-5b4dc18b16ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f99f03-6af2-439f-b7c0-557f788b8837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForSequenceClassification(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 3072)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3072, out_features=4, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "726e83c3-b25f-4f23-8afd-2c0c27fe55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d9827d-d246-4e80-949a-d1f6183445b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df689c72-b298-400e-86fd-8dee3e87d5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3445a6528b51406cad410dd5597724ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a3c24f3d5b459e9b87cd561ede0d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c90b0d04e34cfdbdff1470c07acc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 512\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['article'], truncation=True, max_length=max_len)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442b5da7-df8f-424b-bdbc-aa4605c75581",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f8c2b5a-7903-418f-96c5-1d047e180d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a9c0c7-76f7-457a-a4cf-6aa6c611be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8db7709-c6df-46b4-ae4c-eb2b2c79a56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sequence_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95ffccc1-55e8-4102-b4f8-8d88c2fb0117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_237/4061989769.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights = class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f6cf5e-af92-4090-91b5-b18da0dd2f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4104326c-ddb7-4620-bd5b-101373115936",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0 : 'left', 1: 'lean-left', 2: 'lean-right', 3: 'right'}\n",
    "maptwo = {\"left\": 0,\"lean-left\": 1,\"lean-right\":2,\"right\":3}\n",
    "def make_predictions(model, df):\n",
    "\n",
    "\n",
    "  # Convert summaries to a list\n",
    "  sentences = df.article.tolist()\n",
    "\n",
    "  # Define the batch size\n",
    "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "  # Initialize an empty list to store the model outputs\n",
    "  all_outputs = []\n",
    "\n",
    "  # Process the sentences in batches\n",
    "  for i in range(0, len(sentences), batch_size):\n",
    "      # Get the batch of sentences\n",
    "      batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "      # Tokenize the batch\n",
    "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "      # Perform inference and store the logits\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**inputs)\n",
    "          all_outputs.append(outputs['logits'])\n",
    "\n",
    "  final_outputs = torch.cat(all_outputs, dim=0)\n",
    "  df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "  df['predictions']=df['predictions'].apply(lambda l:label_mapping[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcbb0ecb-015f-4c25-b52a-66b157084dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(df_test):\n",
    "  y_test = df_test.label.round()\n",
    "  y_pred = df_test.predictions.map(maptwo)\n",
    "  print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  print(\"\\nClassification Report:\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "074916a8-6dbe-41ee-9abd-706e467b754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 12950    2\n",
      "13157    2\n",
      "13350    2\n",
      "6300     2\n",
      "5896     2\n",
      "        ..\n",
      "7646     1\n",
      "1035     3\n",
      "11838    0\n",
      "2943     3\n",
      "7727     1\n",
      "Name: label, Length: 718, dtype: int64 and pred 12950    2\n",
      "13157    2\n",
      "13350    2\n",
      "6300     2\n",
      "5896     2\n",
      "        ..\n",
      "7646     1\n",
      "1035     3\n",
      "11838    0\n",
      "2943     3\n",
      "7727     1\n",
      "Name: predictions, Length: 718, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[ 48   0   0   0]\n",
      " [  1 245   5   0]\n",
      " [  0   2 198   5]\n",
      " [  0   0   4 210]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        48\n",
      "           1       0.99      0.98      0.98       251\n",
      "           2       0.96      0.97      0.96       205\n",
      "           3       0.98      0.98      0.98       214\n",
      "\n",
      "    accuracy                           0.98       718\n",
      "   macro avg       0.98      0.98      0.98       718\n",
      "weighted avg       0.98      0.98      0.98       718\n",
      "\n",
      "Balanced Accuracy Score: 0.9808144218203547\n",
      "Accuracy Score: 0.9763231197771588\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12950</th>\n",
       "      <td>At least two Jan. 6 defendants got out from be...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13157</th>\n",
       "      <td>Vice President Kamala Harris and her Democrati...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13350</th>\n",
       "      <td>The Supreme Court has settled the issue of whe...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>Octogenarian President Biden has difficulty fu...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>Democratic lawmakers have been complaining to ...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>News Analysis This presidential campaign is pl...</td>\n",
       "      <td>1</td>\n",
       "      <td>lean-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>Democrat California Gov. Gavin Newsom has said...</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>Harris trounced Trump in August fundraising. C...</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Zyahna Bryant, the Black Lives Matter and “fat...</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7727</th>\n",
       "      <td>Campaign Notebook At a news conference in Flor...</td>\n",
       "      <td>1</td>\n",
       "      <td>lean-left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  label predictions\n",
       "12950  At least two Jan. 6 defendants got out from be...      2  lean-right\n",
       "13157  Vice President Kamala Harris and her Democrati...      2  lean-right\n",
       "13350  The Supreme Court has settled the issue of whe...      2  lean-right\n",
       "6300   Octogenarian President Biden has difficulty fu...      2  lean-right\n",
       "5896   Democratic lawmakers have been complaining to ...      2  lean-right\n",
       "...                                                  ...    ...         ...\n",
       "7646   News Analysis This presidential campaign is pl...      1   lean-left\n",
       "1035   Democrat California Gov. Gavin Newsom has said...      3       right\n",
       "11838  Harris trounced Trump in August fundraising. C...      0        left\n",
       "2943   Zyahna Bryant, the Black Lives Matter and “fat...      3       right\n",
       "7727   Campaign Notebook At a news conference in Flor...      1   lean-left\n",
       "\n",
       "[718 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, classification_report, confusion_matrix\n",
    "make_predictions(model,df_val)\n",
    "\n",
    "get_performance_metrics(df_val)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b752a10-90ea-4112-a317-2d9a4b694fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  total_flos               = 92898810GF\n",
      "  train_loss               =     0.1911\n",
      "  train_runtime            = 2:04:59.32\n",
      "  train_samples            =       5741\n",
      "  train_samples_per_second =      1.531\n",
      "  train_steps_per_second   =      0.191\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(dataset_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(dataset_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d54c8816-45f4-4906-9976-d5fba1fd48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd31f7-e63b-48f6-a929-90e84daa8049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
