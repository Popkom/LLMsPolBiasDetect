{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dcb954f-6306-4480-be4c-a6af231d277e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.10.0\n",
      "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (4.36.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (4.65.0)\n",
      "Collecting accelerate>=0.21.0 (from peft==0.10.0)\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (0.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/miniconda3/lib/python3.10/site-packages (from peft==0.10.0) (0.20.2)\n",
      "Collecting huggingface-hub>=0.17.0 (from peft==0.10.0)\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from peft==0.10.0)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.10.0) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.10.0) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.10.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/miniconda3/lib/python3.10/site-packages (from transformers->peft==0.10.0) (0.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.10.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.10.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.10.0) (1.3.0)\n",
      "Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.1-py3-none-any.whl (468 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, accelerate, peft\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.4.1\n",
      "    Uninstalling safetensors-0.4.1:\n",
      "      Successfully uninstalled safetensors-0.4.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.2\n",
      "    Uninstalling huggingface-hub-0.20.2:\n",
      "      Successfully uninstalled huggingface-hub-0.20.2\n",
      "Successfully installed accelerate-1.4.0 huggingface-hub-0.29.1 peft-0.10.0 safetensors-0.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate==0.34.2\n",
      "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (0.29.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from accelerate==0.34.2) (0.5.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2023.12.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate==0.34.2) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.4.0\n",
      "    Uninstalling accelerate-1.4.0:\n",
      "      Successfully uninstalled accelerate-1.4.0\n",
      "Successfully installed accelerate-0.34.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: huggingface-hub in /opt/miniconda3/lib/python3.10/site-packages (0.29.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub) (2023.11.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (0.5.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.10/site-packages (from transformers==4.45.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2023.11.17)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.20.3 transformers-4.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers==3.1.1\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (2.1.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (1.3.2)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (0.29.1)\n",
      "Requirement already satisfied: Pillow in /opt/miniconda3/lib/python3.10/site-packages (from sentence-transformers==3.1.1) (10.0.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers==3.1.1) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (1.26.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (2023.12.25)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (0.5.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers==3.1.1) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.1.1) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers==3.1.1) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.1.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers==3.1.1) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers==3.1.1) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-3.1.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting protobuf\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "Successfully installed protobuf-5.29.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (1.26.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.7.1)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.4/205.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, requests, pyarrow, propcache, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.3.0 pyarrow-19.0.1 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.10/site-packages (from bitsandbytes) (1.26.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from torch<3,>=2.0->bitsandbytes) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.2-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft==0.10.0\n",
    "!pip install accelerate==0.34.2\n",
    "!pip install --upgrade huggingface-hub\n",
    "!pip install sentencepiece\n",
    "!pip install transformers==4.45.2\n",
    "!pip install sentence-transformers==3.1.1\n",
    "!pip install protobuf\n",
    "!pip install datasets\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec292d42-adff-43f9-9c19-a0a476a4d7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 article       label\n",
      "0      If   loses November’s election, it will be for...        left\n",
      "1      is not a rational choice for conservative Repu...        left\n",
      "2      The outsider candidate has money, a running ma...        left\n",
      "3      No recent Democratic  has faced such fierce in...        left\n",
      "4      The conservative justices have shown they are ...        left\n",
      "...                                                  ...         ...\n",
      "13898  ATLANTA — A Republican-led group is challengin...  lean-right\n",
      "13899  PRAIRIE DU CHIEN, Wis. — A day after Vice  Kam...  lean-right\n",
      "13900  says newly released statistics from Immigratio...  lean-right\n",
      "13901  OPINION: Colombian  Gustavo Petro, in remarks ...  lean-right\n",
      "13902  Vice  Kamala Harris is closing out the race ag...  lean-right\n",
      "\n",
      "[13903 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "login(token='secret')\n",
    "\n",
    "with open('dataset.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def shorten_article(entry):\n",
    "    entry[\"article\"] = entry[\"article\"][:int(len(entry[\"article\"])*0.1)]\n",
    "    cleaned_text = re.sub(r'\\b(Donald|donald|Trump|trump|former|Former|president|President|Joe|joe|Biden|biden)\\b', '', entry[\"article\"])\n",
    "    entry[\"article\"] = cleaned_text.strip()\n",
    "    return entry\n",
    "\n",
    "data = [shorten_article(entry) for entry in data]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#df = df.sample(frac=0.5,random_state=42)\n",
    "print(df)\n",
    "\n",
    "label_mapping = {\"left\": 0, \"lean-left\": 1, \"lean-right\": 2, \"right\": 3}\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "train_size = 0.6\n",
    "temp_size = 0.4\n",
    "val_size = 0.5\n",
    "test_size = 0.5\n",
    "df_train, df_temp = train_test_split(df, train_size=train_size, test_size=temp_size, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, train_size=val_size, test_size=test_size, random_state=42)\n",
    "dataset_train = Dataset.from_pandas(df_train.reset_index(drop=True))\n",
    "dataset_val = Dataset.from_pandas(df_val.reset_index(drop=True))\n",
    "dataset_test = Dataset.from_pandas(df_test.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a9950828-c472-4ef2-b6bb-70238355187f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 8341\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 2781\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'label'],\n",
       "        num_rows: 2781\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': dataset_train,\n",
    "    'val': dataset_val,\n",
    "    'test': dataset_test\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab266e14-244f-4c87-98f5-0818d792a941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "3    0.333893\n",
       "1    0.313512\n",
       "2    0.294689\n",
       "0    0.057907\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8db821b3-52cf-497e-b5c9-25206507be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6432, 0.1188, 0.1264, 0.1116])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (1/df_train.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6a3669dd-3ac2-4e3e-9324-79bf60dfa154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"google/gemma-2-2b\"\n",
    "#model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "439b32f8-58a0-4b95-93af-b49e699c6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, # enable 4-bit quantization\n",
    "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
    "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "967d9147-9cc2-48b8-b901-ae1c690c2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r = 16, # the dimension of the low-rank matrices\n",
    "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd9c8226-0cb2-4ca5-8f59-aa710b7153be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d2f16ab4e14f02a43039782968ef05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Phi3ForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-3.5-mini-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForSequenceClassification(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3SdpaAttention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=4\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ab8719d4-9c59-450f-bc07-9e5f359e3444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForSequenceClassification(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3SdpaAttention(\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2e97fb1e-2eb2-414c-9d82-7c97cfd91a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForSequenceClassification(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3SdpaAttention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=3072, out_features=4, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c5e780fd-010b-4d3a-8a20-28a57bd6a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "74e3d567-0f74-42d8-b027-2d4d3012cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2f0d14a8-8251-4331-9f53-ac6a1f076c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f7d8467b01497fb6b8cd31df882e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1fc8965e3042289118a66b69edcb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35585f5fce504af0a76049e3eaa5e527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2781 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 512\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['article'], truncation=True, max_length=max_len)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True)\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "27262910-22fb-404a-85f7-a2930ebb371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d2c7a17f-e73b-4070-98ec-25e87e512852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9b32780b-1fa2-4b98-ad76-f941a2abf4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "79e8ca6e-8e72-4bb0-8362-5285f90ab99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'sequence_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b9b95764-a455-40dc-b2a8-14883c1a80ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_124/2851958097.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    eval_dataset = tokenized_datasets['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights = class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a34e6e73-5608-4f24-8706-7e3e4ebfa5d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2086' max='2086' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2086/2086 1:54:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.538600</td>\n",
       "      <td>0.442694</td>\n",
       "      <td>0.828119</td>\n",
       "      <td>0.827129</td>\n",
       "      <td>0.829409</td>\n",
       "      <td>0.828119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>0.383454</td>\n",
       "      <td>0.856167</td>\n",
       "      <td>0.856394</td>\n",
       "      <td>0.857005</td>\n",
       "      <td>0.856167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e156ad7f-ae1a-42b0-87e3-288945e39006",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {0 : 'left', 1: 'lean-left', 2: 'lean-right', 3: 'right'}\n",
    "maptwo = {\"left\": 0,\"lean-left\": 1,\"lean-right\":2,\"right\":3}\n",
    "def make_predictions(model, df):\n",
    "\n",
    "\n",
    "  # Convert summaries to a list\n",
    "  sentences = df.article.tolist()\n",
    "\n",
    "  # Define the batch size\n",
    "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
    "\n",
    "  # Initialize an empty list to store the model outputs\n",
    "  all_outputs = []\n",
    "\n",
    "  # Process the sentences in batches\n",
    "  for i in range(0, len(sentences), batch_size):\n",
    "      # Get the batch of sentences\n",
    "      batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "      # Tokenize the batch\n",
    "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
    "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "      # Perform inference and store the logits\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**inputs)\n",
    "          all_outputs.append(outputs['logits'])\n",
    "\n",
    "  final_outputs = torch.cat(all_outputs, dim=0)\n",
    "  df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "  df['predictions']=df['predictions'].apply(lambda l:label_mapping[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a14f1ffa-2d46-4b46-89ed-f3c8cfa0a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_metrics(df_test):\n",
    "  y_test = df_test.label.round()\n",
    "  y_pred = df_test.predictions.map(maptwo)\n",
    "  print(f\"comparing test {y_test} and pred {y_pred}\")\n",
    "\n",
    "  print(\"Confusion Matrix:\")\n",
    "  print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  print(\"\\nClassification Report:\")\n",
    "  print(classification_report(y_test, y_pred))\n",
    "\n",
    "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d2a07947-6d8d-4260-97dd-a1f46aceb396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing test 1975     3\n",
      "10728    1\n",
      "8898     1\n",
      "2210     3\n",
      "13564    2\n",
      "        ..\n",
      "11901    0\n",
      "6363     2\n",
      "6000     2\n",
      "6455     2\n",
      "3684     3\n",
      "Name: label, Length: 2781, dtype: int64 and pred 1975     3\n",
      "10728    1\n",
      "8898     1\n",
      "2210     3\n",
      "13564    2\n",
      "        ..\n",
      "11901    0\n",
      "6363     2\n",
      "6000     2\n",
      "6455     2\n",
      "3684     3\n",
      "Name: predictions, Length: 2781, dtype: int64\n",
      "Confusion Matrix:\n",
      "[[139   1   5   5]\n",
      " [  2 763  93  44]\n",
      " [  5  71 659  69]\n",
      " [  3  30  72 820]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       150\n",
      "           1       0.88      0.85      0.86       902\n",
      "           2       0.79      0.82      0.81       804\n",
      "           3       0.87      0.89      0.88       925\n",
      "\n",
      "    accuracy                           0.86      2781\n",
      "   macro avg       0.87      0.87      0.87      2781\n",
      "weighted avg       0.86      0.86      0.86      2781\n",
      "\n",
      "Balanced Accuracy Score: 0.8696757247203188\n",
      "Accuracy Score: 0.8561668464581086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>An Associated Press smear piece attacking lead...</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10728</th>\n",
       "      <td>The Supreme Court on Friday rejected an emerge...</td>\n",
       "      <td>1</td>\n",
       "      <td>lean-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>Hispanic views on the issue vary widely, and D...</td>\n",
       "      <td>1</td>\n",
       "      <td>lean-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2210</th>\n",
       "      <td>When   released a video where he said, “’s pro...</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13564</th>\n",
       "      <td>HARRISBURG, Pa. — Pennsylvania’s highest court...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>Republican voters don’t really care for the wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>A federal judge in Illinois appointed by  Bara...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>DETROIT      did not gain many sympathy votes ...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6455</th>\n",
       "      <td>Wall Street Journal publisher Almar Latour cal...</td>\n",
       "      <td>2</td>\n",
       "      <td>lean-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>Fox News host Jesse Watters mocked Democrats o...</td>\n",
       "      <td>3</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2781 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  label predictions\n",
       "1975   An Associated Press smear piece attacking lead...      3       right\n",
       "10728  The Supreme Court on Friday rejected an emerge...      1   lean-left\n",
       "8898   Hispanic views on the issue vary widely, and D...      1   lean-left\n",
       "2210   When   released a video where he said, “’s pro...      3       right\n",
       "13564  HARRISBURG, Pa. — Pennsylvania’s highest court...      2  lean-right\n",
       "...                                                  ...    ...         ...\n",
       "11901  Republican voters don’t really care for the wa...      0        left\n",
       "6363   A federal judge in Illinois appointed by  Bara...      2  lean-right\n",
       "6000   DETROIT      did not gain many sympathy votes ...      2  lean-right\n",
       "6455   Wall Street Journal publisher Almar Latour cal...      2  lean-right\n",
       "3684   Fox News host Jesse Watters mocked Democrats o...      3       right\n",
       "\n",
       "[2781 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, classification_report, confusion_matrix\n",
    "make_predictions(model,df_val)\n",
    "\n",
    "get_performance_metrics(df_val)\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cd0571-31df-4656-931b-66f96238f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(model,df_test)\n",
    "\n",
    "get_performance_metrics(df_test)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0af761a6-e435-4070-8206-48f9907c9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =         2.0\n",
      "  total_flos               = 133439458GF\n",
      "  train_loss               =      0.1369\n",
      "  train_runtime            =  3:09:16.07\n",
      "  train_samples            =       11483\n",
      "  train_samples_per_second =       2.022\n",
      "  train_steps_per_second   =       0.253\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = len(dataset_train)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(dataset_train))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8e8acfc-fcff-4948-b447-c06a5115ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb13c7-72eb-4181-8fd4-82eed044f0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
